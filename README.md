# Supplementary codes for the submission "An Investigation of Representation and Allocation Harms in Contrastive Learning"

Our case-study of controlled underrepresentation on `CIFAR10` dataset [[1]](#1) are implemented with three contrastive learning protocol: (1) `SimCLR` [[2]](#2), (2) `SimSIAM` [[3]](#3), and (3) boosted contrastive learning (`BCL`) with `SimCLR` protocol [[4]](#4).  

## References
<a id="1">[1]</a> 
Krizhevsky, A., & Hinton, G. (2009). 
Learning multiple layers of features from tiny images.

<a id="2">[2]</a>
Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020, November). 
A simple framework for contrastive learning of visual representations. 
In International conference on machine learning (pp. 1597-1607). PMLR.

<a id="3">[3]</a>
Chen, X., & He, K. (2021). 
Exploring simple siamese representation learning. 
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 15750-15758).

<a id="4">[4]</a>
Zhou, Z., Yao, J., Wang, Y. F., Han, B., & Zhang, Y. (2022, June). 
Contrastive learning with boosted memorization. 
In International Conference on Machine Learning (pp. 27367-27377). PMLR.
